{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2ef25-4b83-4460-bf1d-3b3dece3e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\\\nathi\\\\Downloads\\\\3 Semester\\\\AMS\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d8a28-2dd8-4db0-b9c5-ffdba8dd3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy.typing as npt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import itertools\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(\n",
    "    ts: npt.ArrayLike, training_size: int\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame] | tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Time series train test split. Performs a single split of the series.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ts: array-like\n",
    "        univariate time series data set\n",
    "\n",
    "    training_size: int\n",
    "        Size of the training set. The test set length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame] | Tuple[np.ndarray, np.ndarray]\n",
    "        A tuple containing the training and test sets,\n",
    "        either as DataFrames or NumPy arrays\n",
    "    \"\"\"\n",
    "    if training_size >= len(ts):\n",
    "        raise ValueError(\"training_size must be < length of series\")\n",
    "\n",
    "    if isinstance(ts, pd.DataFrame):\n",
    "        return ts.iloc[:training_size], ts.iloc[training_size:]\n",
    "\n",
    "    return np.asarray(ts[:training_size]), np.asarray(ts[training_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899c960-5e4f-42cd-bd03-8ac1db1ef86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets_pseudo/Threshold/'\n",
    "names = os.listdir(path)\n",
    "date_str = datetime.datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n",
    "dfs = {}\n",
    "\n",
    "for name in names:\n",
    "    machine, material, component, _ = name.replace('.csv', '').split('_')\n",
    "    tempdf = pd.read_csv(f\"{path}\\\\{name}\") \n",
    "    # tempdf[['Machine', 'Material', 'Component']] = [machine, material, component]\n",
    "    # print(f\"Size of {name}: {tempdf.shape}\")\n",
    "\n",
    "    dfs[name.replace('.csv', '')] = tempdf\n",
    "\n",
    "df = pd.concat(dfs.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be056e20-9bee-4acd-bd1a-b9b3ba1c4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_predict = ['CURRENT|1', 'CURRENT|2', 'CURRENT|3', 'CURRENT|6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f43817-f26d-42bb-8835-2c839a1485d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "contamination_values = [0.01, 0.05, 0.02]\n",
    "n_estimators_list = [50, 100, 150]\n",
    "feature_sets = {\n",
    "    'all_sensors': list(set(df.columns) - set(['Machine', 'Material', 'Component']) - set(['CURRENT|1_Peak', 'CURRENT|2_Peak', 'CURRENT|3_Peak', 'CURRENT|6_Peak'])), \n",
    "    'torques' : ['TORQUE|1', 'TORQUE|2', 'TORQUE|3', 'TORQUE|6']\n",
    "}\n",
    "\n",
    "results = []\n",
    "# names = ['threshold_CMX1_AL_CP1.csv']\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    df_filtered = pd.read_csv(f\"{path}\\\\{name}\")\n",
    "\n",
    "    for column in columns_to_predict:  # adjust if needed\n",
    "        pseudo_label = f\"{column}_Peak\"\n",
    "\n",
    "        for feature_label, features in feature_sets.items():\n",
    "            feature_set = features.copy()\n",
    "            if column not in feature_set:\n",
    "                feature_set.append(column)\n",
    "\n",
    "            # Drop NA\n",
    "            data = df_filtered[feature_set + [pseudo_label]].dropna()\n",
    "            full_index = data.index\n",
    "            X = data[feature_set].drop(columns=[column])\n",
    "            y_full = data[[column]].values.ravel()\n",
    "            y_true = data[pseudo_label].astype(bool)\n",
    "\n",
    "            # Train-test split\n",
    "            train_size = round(0.5 * data.shape[0])\n",
    "            X_train, X_test = ts_train_test_split(X, train_size)\n",
    "            y_train, y_test = ts_train_test_split(y_full, train_size)\n",
    "            y_pseudo = y_true.iloc[train_size:].values.ravel()\n",
    "            idx_train, idx_test = ts_train_test_split(full_index, train_size)\n",
    "\n",
    "            for contamination, n_estimators in itertools.product(contamination_values, n_estimators_list):\n",
    "\n",
    "                plot_dir = f'Plots\\\\IT_{date_str}_{contamination}_{n_estimators}_{name}\\\\' \n",
    "                if not os.path.exists(plot_dir):\n",
    "                    os.makedirs(plot_dir)\n",
    "\n",
    "                model = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=42)\n",
    "                model.fit(X_train)\n",
    "\n",
    "                preds = model.predict(X_test)\n",
    "                y_pred = (preds == -1).astype(bool)\n",
    "\n",
    "                # Evaluation\n",
    "                accuracy = accuracy_score(y_pseudo, y_pred)\n",
    "                precision = precision_score(y_pseudo, y_pred, zero_division=0)\n",
    "                recall = recall_score(y_pseudo, y_pred, zero_division=0)\n",
    "                f1 = f1_score(y_pseudo, y_pred, zero_division=0)\n",
    "                n_peaks = y_pred.sum()\n",
    "\n",
    "                results.append({\n",
    "                    'Dataset': name.replace(\".csv\", \"\"),\n",
    "                    'Column': column,\n",
    "                    'Features': feature_label,\n",
    "                    'Contamination': contamination,\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'Precision': precision,\n",
    "                    'Recall': recall,\n",
    "                    'F1': f1,\n",
    "                    'Detected_Peaks': n_peaks\n",
    "                })\n",
    "\n",
    "                # Plot Actual vs Predicted Peaks\n",
    "                fig, ax = plt.subplots(figsize=(14, 6))\n",
    "                ax.plot(idx_train, y_train, label='Train Data', alpha=0.5, color='blue')\n",
    "                ax.plot(idx_test, y_test, label='Test Data', alpha=0.8, color='green')\n",
    "\n",
    "                # plot as shaded areas\n",
    "                for i, is_peak in enumerate(y_pred):\n",
    "                    if is_peak:\n",
    "                        ax.axvspan(idx_test[i], idx_test[i], color='red', alpha=0.8)\n",
    "\n",
    "                # # Plot as points\n",
    "                # peak_indices = [idx_test[i] for i, is_peak in enumerate(y_pred) if is_peak]\n",
    "                # peak_values = [y_test[i] for i, is_peak in enumerate(y_pred) if is_peak]\n",
    "                # ax.plot(peak_indices, peak_values, 'ro', label='Detected Peaks')\n",
    "\n",
    "                ax.set_title(f\"Detected Peaks in {column} using Isolation Forest cont: {contamination} and est: {n_estimators}\")\n",
    "                ax.set_xlabel(\"Index\")\n",
    "                ax.set_ylabel(\"Current\")\n",
    "                ax.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{name}\\\\{column.replace(\"|\", \"\")}.png')\n",
    "                # plt.savefig(f\"Plots/IT_plot_{column.replace('|', '')}_{feature_label}.png\")\n",
    "                plt.show()\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "results_df = pd.DataFrame(results)  \n",
    "results_df.sort_values(by='F1', ascending=False, inplace=True)\n",
    "display(results_df)\n",
    "\n",
    "results_df.to_csv(f\"Results/IT_results_{date_str}_all.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efa02c-76fc-45af-87e6-ee77a2c5c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity analysis \n",
    "# regression to identify peaks and use those as pseudo labels\n",
    "# use secondary variables to be able to identify the peaks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
